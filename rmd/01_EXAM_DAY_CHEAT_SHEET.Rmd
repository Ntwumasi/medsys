---
title: "ğŸ“– EXAM DAY CHEAT SHEET"
subtitle: "CSCI E-106 Midterm - November 5, 2025"
author: "Your Complete Reference Guide"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: cosmo
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, warning = FALSE, message = FALSE)
```

# ğŸš€ QUICK START - LOAD THESE FIRST

```{r load-libraries}
# ALWAYS run this at the start of your exam
library(MASS)        # boxcox, robust regression
library(car)         # VIF test
library(olsrr)       # Breusch-Pagan, Cook's distance
library(caret)       # Model evaluation
library(glmnet)      # Ridge/Lasso
library(dplyr)       # Data manipulation
```

---

# ğŸ“‹ PROBLEM WORKFLOW CHECKLIST

For every problem, follow this order:

1. âœ… **Read the problem** - What is Y? What are Xs? Seed? Train/test split?
2. âœ… **Load data** - `read.csv()`, check with `str()` and `summary()`
3. âœ… **Handle dummies** - Create dummy variables for categorical data
4. âœ… **Split data** - Use correct seed! 70/30 or as specified
5. âœ… **Build model** - `lm(Y ~ ., data = train)`
6. âœ… **Check diagnostics** - Plots, VIF, Breusch-Pagan
7. âœ… **Fix problems** - Transform, robust, regularization as needed
8. âœ… **Evaluate** - Test data performance
9. âœ… **Write answer** - Explain in words, show work

---

# ğŸ“Š STEP 1: LOAD & EXPLORE DATA

```{r data-exploration}
# Load data
mydata <- read.csv("filename.csv")

# Look at structure
str(mydata)           # Variable types
summary(mydata)       # Summary statistics
head(mydata)          # First few rows
dim(mydata)           # Rows and columns

# Check for missing values
colSums(is.na(mydata))

# Remove NAs if needed
# mydata <- na.omit(mydata)
```

---

# ğŸ·ï¸ STEP 2: DUMMY VARIABLES

## Quick Rule
**Categorical variable with K levels â†’ Create (K-1) dummy variables**

Example: ShelveLoc = {Bad, Good, Medium} â†’ Create 2 dummies

```{r dummy-variables}
# Method 1: Let R handle it (easiest!)
mydata$CatVar <- as.factor(mydata$CatVar)
# R creates dummies automatically in lm()

# Method 2: Manual creation
mydata$Bad <- ifelse(mydata$ShelveLoc == "Bad", 1, 0)
mydata$Good <- ifelse(mydata$ShelveLoc == "Good", 1, 0)
# Medium is reference (both dummies = 0)
```

**Writing the equation:**
```
Y = Î²â‚€ + Î²â‚X + Î²â‚‚(Bad) + Î²â‚ƒ(Good)

Where:
  Bad = 1 if Bad, 0 otherwise
  Good = 1 if Good, 0 otherwise
  Reference: Medium (when both = 0)
```

---

# âœ‚ï¸ STEP 3: TRAIN/TEST SPLIT

```{r train-test}
# Use the EXACT seed from the problem!
set.seed(15)  # Or 1023, 8804 - check problem!

n <- nrow(mydata)
train_idx <- sample(1:n, 0.7 * n)  # 70% train

train_data <- mydata[train_idx, ]
test_data <- mydata[-train_idx, ]

# Verify split
nrow(train_data)  # ~70% of total
nrow(test_data)   # ~30% of total
```

---

# ğŸ“ˆ STEP 4: BUILD MODEL

```{r build-model}
# Linear regression with all predictors
model <- lm(Y ~ ., data = train_data)

# View results
summary(model)

# Key things to note:
# - Coefficients (Estimate)
# - p-values (Pr(>|t|))
# - R-squared
# - F-statistic p-value
```

**What to write:**
```
Model: Y = Î²â‚€ + Î²â‚Xâ‚ + Î²â‚‚Xâ‚‚ + ...

From summary output:
- RÂ² = [value]: Model explains [X%] of variance
- Adjusted RÂ² = [value]
- Significant predictors (p < 0.05): [list]
- Non-significant (p > 0.05): [list]
```

---

# ğŸ” STEP 5: DIAGNOSTICS

## A. Diagnostic Plots

```{r diagnostic-plots}
par(mfrow = c(2, 2))
plot(model)
par(mfrow = c(1, 1))
```

**What each plot shows:**

1. **Residuals vs Fitted**
   - âœ… Good: Random scatter around 0
   - âŒ Bad: Pattern, curve, funnel

2. **Q-Q Plot**
   - âœ… Good: Points on line
   - âŒ Bad: Points curve off (heavy tails)

3. **Scale-Location**
   - âœ… Good: Horizontal red line
   - âŒ Bad: Sloped line (heteroskedasticity)

4. **Residuals vs Leverage**
   - âŒ Watch: Points beyond dashed lines (influential)

---

## B. VIF Test (Multicollinearity)

```{r vif}
library(car)
vif(model)
```

**Rules:**
- VIF < 5: âœ… No problem
- VIF 5-10: âš ï¸ Moderate
- VIF > 10: âŒ Problem!

**Write:**
"All VIF values < 10, no severe multicollinearity"
OR
"Variables X, Y have VIF > 10, indicating multicollinearity"

---

## C. Breusch-Pagan Test (Heteroskedasticity)

```{r breusch-pagan}
library(olsrr)
ols_test_breusch_pagan(model)
```

**Rules:**
- p > 0.05: âœ… Constant variance
- p < 0.05: âŒ Non-constant variance

**Write:**
"Breusch-Pagan test p = [value]. [Accept/Reject] Hâ‚€. Variance is [constant/not constant]."

---

## D. Cook's Distance (Outliers)

```{r cooks-distance}
library(olsrr)
ols_plot_cooksd_bar(model)

# Threshold: 4/n
cooksd <- cooks.distance(model)
influential <- which(cooksd > 4/nrow(train_data))
influential
```

**Write:**
"Observations [numbers] identified as influential (Cook's D > 4/n)"

---

# ğŸ› ï¸ STEP 6: FIX PROBLEMS

## Problem: Non-Normal Residuals â†’ Box-Cox

```{r boxcox}
library(MASS)

# Find optimal lambda
boxcox(model, lambda = seq(-2, 2, 0.1))
bc <- boxcox(model, lambda = seq(-2, 2, 0.1))
lambda <- bc$x[which.max(bc$y)]
lambda

# Transform based on lambda:
# Î» â‰ˆ 0: use log(Y)
# Î» â‰ˆ 0.5: use sqrt(Y)
# Î» â‰ˆ 1: no transformation needed

# Apply log transformation (most common)
train_data$Y_trans <- log(train_data$Y)
test_data$Y_trans <- log(test_data$Y)

# Refit model
model_trans <- lm(Y_trans ~ . - Y, data = train_data)
summary(model_trans)
```

---

## Problem: Outliers â†’ Robust Regression

```{r robust}
library(MASS)

# Huber method
robust_model <- rlm(Y ~ ., data = train_data, psi = psi.huber)
summary(robust_model)

# Check weights (< 1 = downweighted)
weights <- robust_model$w
sort(weights)[1:10]  # Lowest weights
```

---

## Problem: Multicollinearity â†’ Ridge

```{r ridge}
library(glmnet)

x_train <- model.matrix(Y ~ ., data = train_data)[, -1]
y_train <- train_data$Y
x_test <- model.matrix(Y ~ ., data = test_data)[, -1]
y_test <- test_data$Y

# Find best lambda
cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0)
lambda_ridge <- cv_ridge$lambda.min

# Fit ridge model
ridge_model <- glmnet(x_train, y_train, 
                      alpha = 0, lambda = lambda_ridge)
coef(ridge_model)
```

---

## Variable Selection â†’ Lasso

```{r lasso}
# Find best lambda
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1)
lambda_lasso <- cv_lasso$lambda.min

# Fit lasso
lasso_model <- glmnet(x_train, y_train,
                      alpha = 1, lambda = lambda_lasso)
coef(lasso_model)  # Some coefficients = 0 (eliminated)
```

---

## Variable Selection â†’ Stepwise

```{r stepwise}
library(MASS)
step_model <- stepAIC(model, direction = "both", trace = FALSE)
summary(step_model)
```

---

# ğŸ“Š STEP 7: EVALUATE PERFORMANCE

```{r evaluation}
library(caret)

# Training performance
pred_train <- predict(model, train_data)
train_metrics <- postResample(pred_train, train_data$Y)

# Test performance (MOST IMPORTANT!)
pred_test <- predict(model, test_data)
test_metrics <- postResample(pred_test, test_data$Y)

# Compare
rbind(Train = train_metrics, Test = test_metrics)
```

**Write:**
```
Performance:
- Train RMSE = [value], RÂ² = [value]
- Test RMSE = [value], RÂ² = [value]

Robustness:
[If similar] "Model is robust, similar performance on test data"
[If test worse] "Model may be overfitting, test performance degraded"
```

**For transformed models - TRANSFORM BACK!**
```{r transform-back}
# If you used log:
pred_test_trans <- predict(model_trans, test_data)
pred_test_original <- exp(pred_test_trans)

# Calculate on original scale
postResample(pred_test_original, test_data$Y)
```

---

# ğŸ¯ STEP 8: INTERVALS

## Confidence Intervals (for mean)

```{r confidence}
# 95% CI
conf_95 <- predict(model, test_data[1:5, ], 
                   interval = "confidence", level = 0.95)

# 99% CI (wider)
conf_99 <- predict(model, test_data[1:5, ],
                   interval = "confidence", level = 0.99)
conf_99
```

## Prediction Intervals (for individual)

```{r prediction}
pred_99 <- predict(model, test_data[1:5, ],
                   interval = "prediction", level = 0.99)
pred_99
```

**Write:**
"99% CI for first observation: Predicted = [fit], CI: ([lwr], [upr])"

---

# ğŸ“ COMPARING MODELS

```{r compare}
# Get predictions from all models
pred_ols <- predict(model, test_data)
pred_ridge <- predict(ridge_model, newx = x_test)
pred_lasso <- predict(lasso_model, newx = x_test)
pred_robust <- predict(robust_model, test_data)

# Compare RMSE
comparison <- rbind(
  OLS = postResample(pred_ols, test_data$Y),
  Ridge = postResample(as.vector(pred_ridge), test_data$Y),
  Lasso = postResample(as.vector(pred_lasso), test_data$Y),
  Robust = postResample(pred_robust, test_data$Y)
)

# Rank by RMSE
comparison[order(comparison[, "RMSE"]), ]
```

---

# ğŸ”‘ KEY INTERPRETATIONS

## R-squared
"The model explains [RÂ²Ã—100]% of variance in Y"

## RMSE
"On average, predictions are off by [RMSE] units"

## Coefficients
"A one-unit increase in X leads to a [Î²] unit change in Y, holding other variables constant"

## Dummy Variables
"The effect of [category] compared to [reference] is [Î²] units"

## p-values
- p < 0.05: Significant
- p > 0.05: Not significant

---

# ğŸ†˜ TROUBLESHOOTING

**"Error: object not found"**
â†’ Check spelling, run previous code

**"Singular matrix"**
â†’ Perfect multicollinearity, remove duplicate variable

**"NA/NaN/Inf"**
â†’ Check for missing values: `colSums(is.na(mydata))`

**Model won't converge**
â†’ Try scaling: `scale(mydata[, numeric_cols])`

---

# âœ… FINAL CHECKLIST

Before submitting:

- [ ] Used correct seed
- [ ] Split data correctly (70/30 or as specified)
- [ ] Checked all diagnostics
- [ ] Evaluated on TEST data
- [ ] Transformed back if needed
- [ ] Wrote equation in proper format
- [ ] Explained findings in words
- [ ] Showed all work
- [ ] Knitted to PDF/HTML
- [ ] Submitted BOTH .Rmd and PDF

---

# ğŸ“ EMERGENCY CONTACTS

**Technical Support:**
- Phone: (617) 998-8571
- Email: AcademicTechnology@dce.harvard.edu

**Course Staff:**
- hakangogtas@yahoo.com
- rafael_gomeztagle@g.harvard.edu

---

**YOU'VE GOT THIS! ğŸ“**

*Remember: Open book, 3 hours, show your work, explain your answers*

